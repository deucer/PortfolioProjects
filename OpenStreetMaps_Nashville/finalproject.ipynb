{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import codecs\n",
    "import cerberus\n",
    "import sqlite3\n",
    "import schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "OSM_FILE = \"nashville.osm\"  # Replace this with your osm file\n",
    "SAMPLE_FILE = \"nashville_sample.osm\"\n",
    "\n",
    "k = 15 # Parameter: take every k-th top level element\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "            \n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write(bytes('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n', encoding='utf-8'))\n",
    "    output.write(bytes('<osm>\\n  ', encoding = 'utf-8'))\n",
    "\n",
    "    # Write every kth top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    output.write(bytes('</osm>', encoding = 'utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nashville = \"nashville_sample.osm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tags(filename):\n",
    "    tree = ET.parse(filename)\n",
    "    root = tree.getroot()\n",
    "    tag_list = {}\n",
    "    for row in root.iter():\n",
    "        if row.tag not in tag_list:\n",
    "            tag_list[row.tag] = 1\n",
    "        else:\n",
    "            tag_list[row.tag] +=1\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'osm': 1,\n",
       " 'node': 22642,\n",
       " 'tag': 10399,\n",
       " 'way': 3019,\n",
       " 'nd': 26064,\n",
       " 'relation': 34,\n",
       " 'member': 1914}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tags(nashville)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our sample data of Nashville, we have 3019 ways and 10399 tags we'll be mostly focusing on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regular expressions\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "#count the key types\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        if lower.search(element.attrib[\"k\"]):\n",
    "            keys[\"lower\"] += 1\n",
    "        elif lower_colon.search(element.attrib[\"k\"]):\n",
    "            keys[\"lower_colon\"] += 1\n",
    "        elif problemchars.search(element.attrib[\"k\"]):\n",
    "            keys[\"problemchars\"] += 1\n",
    "        else:\n",
    "            keys[\"other\"] +=1\n",
    "        \n",
    "    return keys\n",
    "\n",
    "#locate the colon in the tag\n",
    "def find_colon(element, c_list):\n",
    "    if element.tag == 'tag':\n",
    "        if lower_colon.search(element.attrib[\"k\"]):\n",
    "            if element.attrib[\"k\"] not in c_list:\n",
    "                c_list[element.attrib[\"k\"]] = 1\n",
    "            else:\n",
    "                c_list[element.attrib[\"k\"]] += 1\n",
    "    return c_list\n",
    "\n",
    "#save the tags into a couple of lists, one to count and one to list them all\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    colon_list = {}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "        colon_list = find_colon(element, colon_list)\n",
    "    return keys, colon_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'lower': 7110, 'lower_colon': 3076, 'other': 213, 'problemchars': 0},\n",
      " {'addr:city': 41,\n",
      "  'addr:country': 1,\n",
      "  'addr:housenumber': 53,\n",
      "  'addr:postcode': 51,\n",
      "  'addr:state': 40,\n",
      "  'addr:street': 54,\n",
      "  'addr:suite': 1,\n",
      "  'addr:unit': 2,\n",
      "  'brand:wikidata': 26,\n",
      "  'brand:wikipedia': 26,\n",
      "  'building:color': 1,\n",
      "  'building:levels': 17,\n",
      "  'building:material': 1,\n",
      "  'building:part': 2,\n",
      "  'building:roof': 1,\n",
      "  'building:units': 1,\n",
      "  'change:lanes': 2,\n",
      "  'cycleway:left': 2,\n",
      "  'cycleway:right': 1,\n",
      "  'demolished:highway': 1,\n",
      "  'destination:ref': 12,\n",
      "  'destination:street': 4,\n",
      "  'generator:source': 1,\n",
      "  'gnis:county_id': 44,\n",
      "  'gnis:county_name': 6,\n",
      "  'gnis:created': 44,\n",
      "  'gnis:feature_id': 55,\n",
      "  'gnis:id': 2,\n",
      "  'gnis:import_uuid': 6,\n",
      "  'gnis:reviewed': 6,\n",
      "  'gnis:state_id': 44,\n",
      "  'hgv:national_network': 1,\n",
      "  'historic:name': 1,\n",
      "  'internet_access:fee': 1,\n",
      "  'internet_access:ssid': 1,\n",
      "  'is_in:state': 3,\n",
      "  'junction:ref': 8,\n",
      "  'lanes:backward': 10,\n",
      "  'lanes:both_ways': 4,\n",
      "  'lanes:forward': 10,\n",
      "  'maxspeed:advisory': 6,\n",
      "  'note:lanes': 5,\n",
      "  'note:old_railway_operator': 2,\n",
      "  'placement:forward': 1,\n",
      "  'public_transport:version': 4,\n",
      "  'ref:left': 1,\n",
      "  'ref:right': 1,\n",
      "  'restriction:conditional': 1,\n",
      "  'roof:angle': 1,\n",
      "  'roof:colour': 1,\n",
      "  'roof:height': 1,\n",
      "  'roof:levels': 2,\n",
      "  'roof:material': 1,\n",
      "  'roof:shape': 1,\n",
      "  'source:name': 1,\n",
      "  'tiger:cfcc': 413,\n",
      "  'tiger:county': 414,\n",
      "  'tiger:name_base': 354,\n",
      "  'tiger:name_direction_prefix': 12,\n",
      "  'tiger:name_direction_suffix': 40,\n",
      "  'tiger:name_type': 312,\n",
      "  'tiger:reviewed': 322,\n",
      "  'tiger:separated': 6,\n",
      "  'tiger:source': 8,\n",
      "  'tiger:tlid': 8,\n",
      "  'tiger:upload_uuid': 8,\n",
      "  'tiger:zip': 1,\n",
      "  'tiger:zip_left': 276,\n",
      "  'tiger:zip_right': 272,\n",
      "  'toilets:wheelchair': 1,\n",
      "  'tower:type': 1,\n",
      "  'traffic_signals:direction': 8,\n",
      "  'turn:lanes': 4})\n"
     ]
    }
   ],
   "source": [
    "keys = process_map(nashville)\n",
    "pprint.pprint(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No problem characters is a relief but let's see if we can find some other errors in our data. Sadly, I went through all of the \"addr:street\" tags looking for errors and only found one. **Rosa Parks Blvd** used the abbreviated version of Boulevard. This is an easily correctable fix (even manually!) but we'll continue on with our corrective functions to finish out our analysis. This data looks clean overall - postal codes, street names, abbreviations, street directions - all look in good shape. The open street map users for the city of Nashville have been busy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "osmfile = \"nashville_sample.osm\"\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Circle\", \"Lane\", \n",
    "            \"Road\", \"Trail\", \"Parkway\", \"Expressway\", \"Highway\", \"Tunnel\", \"Park\", \"Plaza\", \"Pike\", \"Bridge\", \"School\"]\n",
    "\n",
    "# This will look for the values on the left and ultimately replace with those on the right\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Hwy\": \"Highway\",\n",
    "            \"Hwy.\": \"Highway\",\n",
    "           #This should rename Rosa L Parks Blvd => Rosa L Parks Boulevard\n",
    "            \"Blvd\": \"Boulevard\",\n",
    "            \"Blvd.\": \"Boulevard\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Rd\": \"Road\",\n",
    "            \"Pkwy\": \"Parkway\",\n",
    "            \"PKWY\": \"Parkway\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Ave.\": \"Avenue\",\n",
    "            \"S\": \"South\",\n",
    "            \"N\": \"North\",\n",
    "            \"W\": \"West\",\n",
    "            \"E\": \"East\",\n",
    "            \"S.\": \"South\",\n",
    "            \"N.\": \"North\",\n",
    "            \"W.\": \"West\",\n",
    "            \"E.\": \"East\",\n",
    "            }\n",
    "\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for _, elem in ET.iterparse(osmfile):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                audit_street_type(street_types, tag.attrib['v'])\n",
    "                    \n",
    "    osm_file.close()\n",
    "    return street_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_name(name, mapping):\n",
    "    m = street_type_re.search(name)\n",
    "    fixed_name = name\n",
    "    if m:\n",
    "        # check if the street type is a key in the mapping dictionary:\n",
    "        if m.group() in mapping.keys():\n",
    "            fixed_street_type = mapping[m.group()]\n",
    "            fixed_name = street_type_re.sub(fixed_street_type, name)\n",
    "    return fixed_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rosa L Parks Blvd => Rosa L Parks Boulevard\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    st_types = audit(osmfile)\n",
    "    #pprint.pprint(dict(st_types))\n",
    "\n",
    "    for st_type, ways in st_types.items():\n",
    "        for name in ways:\n",
    "            if name == \"Rosa L Parks Blvd\":\n",
    "                fixed_name = update_name(name, mapping)\n",
    "                print(name, \"=>\", fixed_name)\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the **Blvd** has been extended to Boulevard. We have completed our 1 fix! Hoorah!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Processed nashville_sample Map\n"
     ]
    }
   ],
   "source": [
    "#We're using the sample data for faster load and validation times\n",
    "OSM_PATH = \"nashville_sample.osm\"\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "    \n",
    "    if element.tag == 'node':\n",
    "        for key in element.attrib.keys():\n",
    "            if key in node_attr_fields:\n",
    "                node_attribs[key] = element.attrib[key]\n",
    "        for child in element:\n",
    "            if child.tag == 'tag':\n",
    "                if problem_chars.search(child.attrib['k']):\n",
    "                    pass\n",
    "                else:\n",
    "                    tags_list = {}\n",
    "                    tags_list['id'] = element.attrib['id']\n",
    "                    if child.attrib['k'] == 'name:en':\n",
    "                        tags_list['value'] = update_name(child.attrib['v'], mapping)\n",
    "                    else:\n",
    "                        tags_list['value'] = child.attrib['v']\n",
    "                    if LOWER_COLON.search(child.attrib['k']):\n",
    "                        colon_position = child.attrib['k'].find(':')\n",
    "                        tags_list['key'] = child.attrib['k'][colon_position+1:]\n",
    "                        tags_list['type'] = child.attrib['k'][:colon_position]\n",
    "                    else:\n",
    "                        tags_list['key'] = child.attrib['k']\n",
    "                        tags_list['type'] = 'regular'\n",
    "                    tags.append(tags_list)    \n",
    "        \n",
    "    if element.tag == 'way':\n",
    "        for key in element.attrib.keys():\n",
    "            if key in way_attr_fields:\n",
    "                way_attribs[key] = element.attrib[key]\n",
    "        position = 0\n",
    "        for child in element:\n",
    "            \n",
    "            if child.tag == 'nd':\n",
    "                way_nodes_list = {}\n",
    "                way_nodes_list['id'] = element.attrib['id']\n",
    "                way_nodes_list['node_id'] = child.attrib['ref']\n",
    "                way_nodes_list['position'] = position\n",
    "                position += 1\n",
    "                way_nodes.append(way_nodes_list)\n",
    "            if child.tag == 'tag':\n",
    "                if problem_chars.search(child.attrib['k']):\n",
    "                    pass\n",
    "                else:\n",
    "                    tags_list = {}\n",
    "                    tags_list['id'] = element.attrib['id']\n",
    "                    if child.attrib['k'] == 'name:en':\n",
    "                        tags_list['value'] = update_name(child.attrib['v'], mapping)\n",
    "                    else:\n",
    "                        tags_list['value'] = child.attrib['v']\n",
    "                    if LOWER_COLON.search(child.attrib['k']):\n",
    "                        colon_position = child.attrib['k'].find(':')\n",
    "                        tags_list['key'] = child.attrib['k'][colon_position+1:]\n",
    "                        tags_list['type'] = child.attrib['k'][:colon_position]\n",
    "                    else:\n",
    "                        tags_list['key'] = child.attrib['k']\n",
    "                        tags_list['type'] = 'regular'\n",
    "                    tags.append(tags_list)  \n",
    "            \n",
    "    if element.tag == 'node':\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    elif element.tag == 'way':\n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.items())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, bytes) else v) for k, v in row.items()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "    \n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w', encoding=\"utf-8\") as nodes_file, \\\n",
    "        codecs.open(NODE_TAGS_PATH, 'w', encoding=\"utf-8\") as nodes_tags_file, \\\n",
    "        codecs.open(WAYS_PATH, 'w', encoding=\"utf-8\") as ways_file, \\\n",
    "        codecs.open(WAY_NODES_PATH, 'w', encoding=\"utf-8\") as way_nodes_file, \\\n",
    "        codecs.open(WAY_TAGS_PATH, 'w', encoding=\"utf-8\") as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "\n",
    "# Note: Validation is ~ 10X slower. For the project consider using a small\n",
    "# sample of the map when validating.\n",
    "    \n",
    "process_map(OSM_PATH, validate=True)\n",
    "print('Successfully Processed {} Map'.format(OSM_PATH[:-4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our five (5) csv files have been written using the provided schema, we can load them into a sqlite database to perform some further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "osmdb = 'nashville_osm.db'\n",
    "\n",
    "connection = sqlite3.connect(osmdb)\n",
    "write_cursor = connection.cursor()\n",
    "write_cursor.execute('''\n",
    "                    CREATE TABLE nodes(id INTEGER, lat TEXT, lon TEXT, user TEXT, uid INTEGER, version TEXT, changeset TEXT, timestamp TEXT)''')\n",
    "\n",
    "connection.commit()\n",
    "\n",
    "with open('nodes.csv', 'r', encoding=\"utf-8\") as csvfile:\n",
    "    middleman = csv.DictReader(csvfile) # comma is default delimiter\n",
    "    to_db = [(i['id'], i['lat'], i['lon'], i['user'], i[\"uid\"], i[\"version\"],i[\"changeset\"],i[\"timestamp\"]) for i in middleman]\n",
    "\n",
    "write_cursor.executemany(\"INSERT INTO nodes (id, lat, lon, user, uid, version, changeset, timestamp) VALUES (?,?,?,?,?,?,?,?);\", to_db)\n",
    "\n",
    "connection.commit()\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect(osmdb)\n",
    "write_cursor = connection.cursor()\n",
    "write_cursor.execute('''\n",
    "                    CREATE TABLE nodes_tags(id INTEGER, key TEXT, value TEXT, type TEXT)''')\n",
    "\n",
    "connection.commit()\n",
    "\n",
    "with open('nodes_tags.csv', 'r', encoding=\"utf-8\") as csvfile:\n",
    "    middleman = csv.DictReader(csvfile) # comma is default delimiter\n",
    "    to_db = [(i['id'], i['key'], i['value'], i['type']) for i in middleman]\n",
    "\n",
    "write_cursor.executemany(\"INSERT INTO nodes_tags(id, key, value, type) VALUES (?,?,?,?);\", to_db)\n",
    "\n",
    "connection.commit()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect(osmdb)\n",
    "write_cursor = connection.cursor()\n",
    "write_cursor.execute('''\n",
    "                    CREATE TABLE ways(id INTEGER, user TEXT, uid TEXT, version TEXT, changeset TEXT, timestamp TEXT)''')\n",
    "\n",
    "connection.commit()\n",
    "\n",
    "with open('ways.csv', 'r', encoding=\"utf-8\") as csvfile:\n",
    "    middleman = csv.DictReader(csvfile) # comma is default delimiter\n",
    "    to_db = [(i['id'], i['user'], i['uid'], i['version'], i[\"changeset\"], i[\"timestamp\"]) for i in middleman]\n",
    "\n",
    "write_cursor.executemany(\"INSERT INTO ways (id, user, uid, version, changeset, timestamp) VALUES (?,?,?,?,?,?);\", to_db)\n",
    "\n",
    "connection.commit()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect(osmdb)\n",
    "write_cursor = connection.cursor()\n",
    "write_cursor.execute('''\n",
    "                    CREATE TABLE ways_tags(id INTEGER, key TEXT, value TEXT, type TEXT)''')\n",
    "\n",
    "connection.commit()\n",
    "\n",
    "with open('ways_tags.csv', 'r', encoding=\"utf-8\") as csvfile:\n",
    "    middleman = csv.DictReader(csvfile) # comma is default delimiter\n",
    "    to_db = [(i['id'], i['key'], i['value'], i['type']) for i in middleman]\n",
    "\n",
    "write_cursor.executemany(\"INSERT INTO ways_tags(id, key, value, type) VALUES (?,?,?,?);\", to_db)\n",
    "\n",
    "connection.commit()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect(osmdb)\n",
    "write_cursor = connection.cursor()\n",
    "write_cursor.execute('''\n",
    "                    CREATE TABLE ways_nodes(id INTEGER, node_id INTEGER, position INTEGER)''')\n",
    "\n",
    "connection.commit()\n",
    "\n",
    "with open('ways_nodes.csv', 'r', encoding=\"utf-8\") as csvfile:\n",
    "    middleman = csv.DictReader(csvfile) # comma is default delimiter\n",
    "    to_db = [(i['id'], i['node_id'], i['position']) for i in middleman]\n",
    "\n",
    "write_cursor.executemany(\"INSERT INTO ways_nodes(id, node_id, position) VALUES (?,?,?);\", to_db)\n",
    "\n",
    "connection.commit()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's explore our newly created database (nashville_osm.db) and some of our new tables we created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**File Sizes:**<br>\n",
    "nashville.osm => 75,813 KB<br>\n",
    "nashville_sample.osm => 5,104 KB<br>\n",
    "nodes.csv => 1,880 KB<br>\n",
    "nodes_tags.csv => 60 KB<br>\n",
    "ways.csv => 179 KB<br>\n",
    "ways_nodes.csv => 614 KB<br>\n",
    "ways_tags.csv => 292 KB\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Nodes**<br>\n",
    "sqlite > SELECT COUNT(*) FROM NODES;<br>\n",
    "***22642***<br>\n",
    "<br>\n",
    "**Number of Ways**<br>\n",
    "sqlite > SELECT COUNT(*) FROM WAYS;<br>\n",
    "***3019***<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Unique Users**<br>\n",
    "sqlite > SELECT COUNT(DISTINCT(e.uid))<br>\n",
    "FROM (SELECT uid FROM nodes UNION ALL SELECT uid FROM ways) e;<br>\n",
    "***1044***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top 10 Contributing Users**<br>\n",
    "sqlite> SELECT e.user, COUNT(*) as num<br>\n",
    "FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e<br>\n",
    "GROUP BY e.user<br>\n",
    "ORDER BY num DESC<br>\n",
    "LIMIT 10;<br>\n",
    "<br>\n",
    ">Gedwards724 | 10706<br>\n",
    ">wward | 3135<br>\n",
    ">woodpeck_fixbot | 726<br>\n",
    ">bobby22 | 637<br>\n",
    ">Tom_Holland | 629<br>\n",
    ">greggerm | 381<br>\n",
    ">ChesterKiwi | 361<br>\n",
    ">maxerickson | 360<br>\n",
    ">42429 | 312<br>\n",
    ">Rub21 | 254<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top 10 Amenities in Nashville**<br>\n",
    "sqlite> SELECT value, COUNT(*) as num<br>FROM nodes_tags<br>WHERE key='amenity'<br>GROUP BY value<br>ORDER BY num DESC<br> LIMIT 10;<br>\n",
    "<br>\n",
    "> place_of_worship | 40 <= Not a surprise! <br>\n",
    "> restaurant | 10<br>\n",
    "> school | 8<br>\n",
    "> cafe | 7<br>\n",
    "> fast_food | 6<br>\n",
    "> fountain | 5<br>\n",
    "> bench | 5<br>\n",
    "> bar | 5<br>\n",
    "> fuel | 4<br>\n",
    "> pub | 2<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "https://stackoverflow.com/questions/15356641/how-to-write-xml-declaration-using-xml-etree-elementtree<br>\n",
    "https://stackoverflow.com/questions/30418481/error-dict-object-has-no-attribute-iteritems<br>\n",
    "https://stackoverflow.com/questions/53089403/how-to-fix-python3-errror-name-unicode-is-not-defined<br>\n",
    "https://stackoverflow.com/questions/28583565/str-object-has-no-attribute-decode-python-3-error<br>\n",
    "https://medium.com/ibm-data-science-experience/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
